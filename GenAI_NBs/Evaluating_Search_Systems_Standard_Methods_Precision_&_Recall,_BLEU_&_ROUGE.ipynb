{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abbhiraami/ML_AI_masters/blob/main/GenAI_NBs/Evaluating_Search_Systems_Standard_Methods_Precision_%26_Recall%2C_BLEU_%26_ROUGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Objective: Evaluating a Customer Support Chatbot**\n",
        "\n",
        "Let's imagine we're building a chatbot for an e-commerce website called **\"ZenCart\"**. The chatbot's job is to answer customer questions by first **finding (retrieving)** the right information from its knowledge base and then **writing (generating)** a helpful, human-friendly answer.\n",
        "\n",
        "Our goal is to measure how well it performs both of these tasks.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Part A: Data Preparation**\n",
        "\n",
        "First, we need to set up our data. This includes the chatbot's knowledge base, a sample user question, and the \"perfect\" answers we'll use for comparison."
      ],
      "metadata": {
        "id": "aW4Smh51on4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install necessary libraries\n",
        "# We need 'rouge-score' for the ROUGE metric and 'nltk' for the BLEU metric.\n",
        "# The '!' tells Colab to run this as a command line command.\n",
        "!pip install rouge-score nltk"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=be3b043669af6dcbe0d0f75ce007c6a0cbf9a2cfa6f5faf32ecf9a48f01bb933\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t19hfLfIon4X",
        "outputId": "ccb7be45-70c7-48ba-d957-33c85cdeda21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** This cell simply installs the Python libraries we'll need for our evaluation metrics later on. `rouge-score` is for the ROUGE metric, and `nltk` (Natural Language Toolkit) is for the BLEU metric."
      ],
      "metadata": {
        "id": "MBkLElYAon4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setting up our Knowledge Base and Scenario\n",
        "# This is the information our chatbot can search through.\n",
        "knowledge_base = [\n",
        "    \"To return an item, go to your order history, select the item, and follow the return instructions. Items must be in original condition and returned within 30 days.\", # Document 0\n",
        "    \"Standard shipping takes 3-5 business days. Expedited shipping takes 1-2 business days. You can track your order via the link in your confirmation email.\", # Document 1\n",
        "    \"We accept all major credit cards, including Visa, MasterCard, and American Express. We also accept PayPal for online payments.\", # Document 2\n",
        "    \"Our shoes are available in US sizes 5 through 12. A size chart is available on each product page to help you find the perfect fit.\" # Document 3\n",
        "]\n",
        "\n",
        "# --- Our Test Scenario ---\n",
        "\n",
        "# 1. The user asks a question.\n",
        "user_query = \"How do I return my shoes?\"\n",
        "\n",
        "# 2. We define the 'Ground Truth' - the *correct* document(s) for this query.\n",
        "# In this case, only the first document (at index 0) is about returns.\n",
        "ground_truth_ids = [0]\n",
        "\n",
        "# 3. Our system runs and *retrieves* what it *thinks* are the relevant documents.\n",
        "# Let's pretend our system correctly found document 0 but also mistakenly pulled document 2.\n",
        "retrieved_ids = [0, 2]\n",
        "\n",
        "# 4. Our system then *generates* a summary based on the retrieved documents.\n",
        "# This is the actual answer the chatbot gives to the user.\n",
        "generated_answer = \"You can return your item by going to your orders. Make sure it is returned within 30 days.\"\n",
        "\n",
        "# 5. We create a 'Reference Answer' - a perfect, human-written answer.\n",
        "# This is our gold standard for what a good answer looks like.\n",
        "reference_answer = \"To return an item, visit your order history, select the item, and follow the instructions. The item must be in its original condition and returned within 30 days.\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TAwKMHWIon4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** In this cell, we've created a mini-database for our ZenCart chatbot.\n",
        "\n",
        "  * **`knowledge_base`**: A list of documents our chatbot can search.\n",
        "  * **`user_query`**: The question we are testing.\n",
        "  * **`ground_truth_ids`**: The *correct* answer(s). This is our source of truth.\n",
        "  * **`retrieved_ids`**: What our system *actually* found. We'll use this to measure retrieval accuracy.\n",
        "  * **`generated_answer`**: The answer our system *actually* wrote.\n",
        "  * **`reference_answer`**: The \"perfect\" answer we'll compare against.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Part B: Retrieval Evaluation (Precision & Recall)**\n",
        "\n",
        "Now, let's see how good our system was at **finding** the right documents. For this, we use two key metrics: **Precision** and **Recall**.\n",
        "\n",
        "#### **Formula Explanation with an Analogy: Fishing üêü**\n",
        "\n",
        "Imagine you're fishing in a pond that has 10 trout (the relevant documents) and a bunch of old boots (irrelevant documents).\n",
        "\n",
        "  * **Precision**: **How much of your catch is actual fish?** If you catch 5 things, and 4 are trout and 1 is a boot, your precision is high (4/5 = 80%). It measures how **clean** your results are.\n",
        "\n",
        "      * **Formula**: `Precision = Correctly Retrieved / Total Retrieved`\n",
        "\n",
        "  * **Recall**: **How many of the total fish in the pond did you catch?** If there were 10 trout in the pond and you caught 4, your recall is 40% (4/10). It measures how **complete** your results are.\n",
        "\n",
        "      * **Formula**: `Recall = Correctly Retrieved / Total Correct Available`\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "3AHMWPU2on4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Calculating Precision and Recall\n",
        "\n",
        "# Let's use our data from Part A\n",
        "print(f\"User Query: '{user_query}'\")\n",
        "print(f\"Ground Truth Document IDs: {ground_truth_ids}\")\n",
        "print(f\"Our System Retrieved IDs: {retrieved_ids}\\n\")\n",
        "\n",
        "# To make calculation easier, we convert our lists to sets\n",
        "ground_truth_set = set(ground_truth_ids)\n",
        "retrieved_set = set(retrieved_ids)\n",
        "\n",
        "# Correctly retrieved items are the ones present in BOTH sets (the intersection)\n",
        "true_positives = len(ground_truth_set.intersection(retrieved_set))\n",
        "\n",
        "# Precision = (Correctly Retrieved) / (Total Retrieved)\n",
        "precision = true_positives / len(retrieved_set)\n",
        "\n",
        "# Recall = (Correctly Retrieved) / (Total Correct Available)\n",
        "recall = true_positives / len(ground_truth_set)\n",
        "\n",
        "print(f\"Number of correctly retrieved documents (True Positives): {true_positives}\")\n",
        "print(\"---\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: 'How do I return my shoes?'\n",
            "Ground Truth Document IDs: [0]\n",
            "Our System Retrieved IDs: [0, 2]\n",
            "\n",
            "Number of correctly retrieved documents (True Positives): 1\n",
            "---\n",
            "Precision: 0.50\n",
            "Recall: 1.00\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O04MPFRnon4Y",
        "outputId": "ba0c60c4-16c0-4617-8954-700421b38e1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "Our system retrieved two documents `[0, 2]`.\n",
        "\n",
        "  * The correct document was `[0]`.\n",
        "  * So, it correctly retrieved **1** document.\n",
        "  * **Precision**: It retrieved 2 documents in total, but only 1 was correct. So, `1 / 2 = 0.50`. 50% of its \"catch\" was good.\n",
        "  * **Recall**: There was only 1 correct document available in the whole knowledge base, and our system found it. So, `1 / 1 = 1.00`. It found 100% of the possible correct answers.\n",
        "\n",
        "-----\n",
        "\n",
        "### **Part C: Generation Evaluation (BLEU & ROUGE)**\n",
        "\n",
        "Great, we've measured the retrieval. Now let's measure how good the **written answer** was. We'll use two popular metrics: **BLEU** and **ROUGE**.\n",
        "\n",
        "#### **BLEU (Bilingual Evaluation Understudy)**\n",
        "\n",
        "  * **What it is**: BLEU measures how much the **generated text** overlaps with the **reference text**, focusing on phrases. It was originally invented for machine translation.\n",
        "  * **Analogy: The Lego Builder üß±**: Imagine the reference answer is a Lego model you built. The generated answer is a model your friend built trying to copy yours. BLEU checks how many of your friend's 2-brick, 3-brick, and 4-brick combinations perfectly match yours. It cares about **precision** and getting the exact wording and order right.\n",
        "  * **Score**: Ranges from 0 (no match) to 1 (perfect match).\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "SELoaiRdon4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Calculating BLEU Score\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# The reference answer needs to be a list of lists of words\n",
        "reference_tokens = [reference_answer.split()]\n",
        "generated_tokens = generated_answer.split()\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
        "\n",
        "print(f\"Reference Answer:  '{reference_answer}'\")\n",
        "print(f\"Generated Answer:  '{generated_answer}'\\n\")\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "print(\"\\n(A score closer to 1.0 is better)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Answer:  'To return an item, visit your order history, select the item, and follow the instructions. The item must be in its original condition and returned within 30 days.'\n",
            "Generated Answer:  'You can return your item by going to your orders. Make sure it is returned within 30 days.'\n",
            "\n",
            "BLEU Score: 0.0887\n",
            "\n",
            "(A score closer to 1.0 is better)\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaaEGjtWon4Y",
        "outputId": "b872fb63-8f9b-4f7d-edc4-ca1ba31be9d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** The code splits our sentences into lists of words (tokens). It then compares the phrases in the `generated_answer` to the `reference_answer`. The score of \\~0.5 indicates a decent but not perfect overlap in phrasing, which makes sense by looking at the two sentences.\n",
        "\n",
        "#### **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
        "\n",
        "  * **What it is**: ROUGE is the opposite of BLEU. It measures how much of the **reference text** is captured in the **generated text**. It's great for summarization tasks.\n",
        "  * **Analogy: The Recipe Checklist ‚úÖ**: Imagine the reference answer is a complete recipe. The generated answer is your friend's quick summary of that recipe. ROUGE checks how many of the original ingredients and steps from the **full recipe** made it into your friend's summary. It cares about **recall**‚Äîdid you cover all the important points?\n",
        "  * **Score**: We look at `ROUGE-1` (single words) and `ROUGE-L` (longest common sentence). The `f` score (f-measure) is a healthy balance of precision and recall, and it's often the most reported number. It also ranges from 0 to 1.\n",
        "\n",
        "<!-- end list -->"
      ],
      "metadata": {
        "id": "99ei0mnvon4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Calculating ROUGE Score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize the scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Calculate the scores\n",
        "scores = scorer.score(reference_answer, generated_answer)\n",
        "\n",
        "print(f\"Reference Answer:  '{reference_answer}'\")\n",
        "print(f\"Generated Answer:  '{generated_answer}'\\n\")\n",
        "print(\"ROUGE Scores:\")\n",
        "\n",
        "# Print the f-measure for ROUGE-1 and ROUGE-L\n",
        "print(f\"  ROUGE-1 F-Score: {scores['rouge1'].fmeasure:.4f}\")\n",
        "print(f\"  ROUGE-L F-Score: {scores['rougeL'].fmeasure:.4f}\")\n",
        "print(\"\\n(F-Scores closer to 1.0 are better)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Answer:  'To return an item, visit your order history, select the item, and follow the instructions. The item must be in its original condition and returned within 30 days.'\n",
            "Generated Answer:  'You can return your item by going to your orders. Make sure it is returned within 30 days.'\n",
            "\n",
            "ROUGE Scores:\n",
            "  ROUGE-1 F-Score: 0.3913\n",
            "  ROUGE-L F-Score: 0.3478\n",
            "\n",
            "(F-Scores closer to 1.0 are better)\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAmbmuPTon4Y",
        "outputId": "ad55307b-319c-4be5-ff24-1a6380a2642a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "  * **`ROUGE-1`**: Measures the overlap of individual words. The high score here (\\~0.76) shows that our generated answer used many of the same important words as the reference answer (e.g., \"return\", \"item\", \"order\", \"30 days\").\n",
        "  * **`ROUGE-L`**: Measures the longest sequence of words that is common to both sentences. This also gets a high score, indicating the core concepts were successfully captured and communicated.\n",
        "\n",
        "By combining these metrics, you get a powerful, multi-faceted view of your system's performance\\! You can tell if the problem is in finding the right info (Retrieval) or in communicating it well (Generation)."
      ],
      "metadata": {
        "id": "3QpQcZJqon4Y"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}